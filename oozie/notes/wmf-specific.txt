If we were to use Hadoop/Oozie for the XML metadata and content
dumps, we would need to fix up the following:

1) Php, mediawiki, mysqlclient etc all would need to be on every
   hadoop node.

2) Every hadoop node would become a scap3 client of the dump scripts.

3) The mediawiki manifests currently set up and run apache, even
   though once I had fixed them up so that the snapshot hosts were
   not forced to run apache without cause.  This must be fixed
   before these roles are applied to the hadoop cluster.

4) A thorough security audit would need to be done on the hadoop
   cluster to understand who has access to which nodes with what
   restrictions and how those restrictions are enforced.  In
   particular, I have read that, on an installation where kerberos
   credentials are not used, any user may effectively read and write
   anywhere on the hadoop filesystem.  It seems that according to
   the Cloudera docs:
   https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Security-Guide/cdh5sg_hadoop_security_intro.html
   this is still the case.  We would need to see about this!

5) The analytics cluster is firewalled off from the rest of
   production for various reasons.  We would need to see if and
   how that would impact dumps production and service.

6) Dumps are produced with a fixed time schedule; they cannot
   be run on a "best effort" basis.  This means that dump jobs
   would have to be run in a queue with guaranteed resources.
   Additional hardware would undoubtedly have to be deployed.
   We would have to be sure that queries by researchers or
   analysts could not adversely impact the run, independently
   of resource management from within Yarn/Oozie, as well.
   This includes accounting for reruns of research and analytics
   queries that fail for whatever reason, as well as dump reruns
   when there are MediaWiki code issues, database server problems,
   and so on.

7) Of course revision content is not now accessible from within
   Hadoop.  For full history we are talking about a lot of
   storage space.  I have no idea what this would mean as far
   as load on the Hadoop servers in practical terms.

8) As I understand it, the edit history reconstruction project
   is not altogether complete, having some edge cases that
   remain problematic.  We would need to look into those in
   detail.  We should make sure all other metadata produced
   in the XML "stubs" dumps is available from the edit tables
   now generated by analytics.

9) We want all dump users to be able to monitor the progress
   of these jobs.  That means public access.  I don't know that
   this is desired for analytics and research jobs generally.
   One approach here is to keep the web interface restricted
   and provide an API (planned in any case as part of the
   dumps rewrite) for dumps users to query for the status of
   any job, run, or project.

To be discussed:

   The "Sanitarium" project now uses a newly revitalized python
   script; I understand that this could be used to filter data
   being introduced into Hadoop.  Are there plans for this,
   does this already happen?  How are hidden/suppressed edits
   handled?  What about page deletions, hidden user names, and
   so on?

   What sort of time frame are we looking at here?
